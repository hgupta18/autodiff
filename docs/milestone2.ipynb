{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L9gbEZNpruFF"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "[Automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) is a method for numerically finding the derivative of a function at a given point. It can be used to find derivatives of complex functions where computing the symbolic derivative can be impossible or computationally costly.\n",
    "\n",
    "Automatic differentiation is more accurate than other numerical differentiation methods such as the [finite difference method](https://en.wikipedia.org/wiki/Finite_difference_method). The finite difference method attempts to find the derivative of a function at a given point by adding a small perturbation ($\\epsilon$):\n",
    "\n",
    "$$ \\frac{\\partial f}{\\partial x} \\approx \\frac{f(x+\\epsilon) - f(x)}{\\epsilon} $$\n",
    "\n",
    "When the perturbation is too large, the estimate for the derivative is not accurate. When the perturbation is too small, it starts to amplify floating point errors. \n",
    "\n",
    "Automatic differentiation achieves high accuracy while avoiding amplified floating point errors by (1) breaking down the function into a sequence of elementary functions (e.g., sin, cos, log, and exp), (2) calculating the exact derivation of these elementary functions, (3) and finally combining them using the [chain rule](https://en.wikipedia.org/wiki/Chain_rule), the [product rule](https://en.wikipedia.org/wiki/Product_rule), and simple mathematical operations (such as addition and multiplication). \n",
    "\n",
    "Given its speed and precision, automatic differentiation is popular within the field of computational science where it has numerous applications. This software package as an implementation of automatic differentiation using Python.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dCO1uO16B3K2"
   },
   "source": [
    "# Background\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Automatic differentiation allows us to compute the true analytic derivative of a function to machine precision. At a high level, this is done by breaking down complex functions into their elementary components and propogating their derivative via the chain rule. Other methods of finding derivatives include symbolic differentiation and finite fifference. Symbolic differentiation is also accurate to machine precision but is more computationally costly and its implementation is more complex. Finite difference is easier and less costly to implement but can quickly lead to floating point errors. As such, automatic differentiation is a more precise and lightweight methodology to use.\n",
    "\n",
    "## Some Calculus\n",
    "\n",
    "The [product rule](https://en.wikipedia.org/wiki/Product_rule) is used to find the derivative of the product of two or more functions. In its simplest form, if $f$ and $g$ are functions, the derivative of their product is given by the following equation: \n",
    "\n",
    "$$ [f(x)g(x)]' = f'(x)g(x)+f(x)g'(x) $$\n",
    "\n",
    "The chain rule is used for computing the derivative of the composition of two or more functions. In its simplest form, if $f$ and $g$ are functions, the derivative of their composition is given by the following equation:\n",
    "\n",
    "$$ [f(g(x))]' = f'(g(x))*g'(x) $$\n",
    "\n",
    "## Modes of Automatic Differentiation\n",
    "\n",
    "The automatic differentiation method can be implemented in two ways depending on how the chain rule is utilized. Consider the formulation:\n",
    "\n",
    "$$ f(x) = g_3(g_2(g_1(x))) $$\n",
    "\n",
    "Using the chain rule, this function's derivative at point $x = a$ is computed as:\n",
    "\n",
    "$$ f'(a) = g_3'(.)*g_2'(.)*g_1'(a) $$\n",
    "\n",
    "The forward mode of automatic differentiation recursively propagates the calculated derivative from the right: first calculates $g_1'(a)$, then $g_2'(.)$, then $g_3'(.)$, and so on. \n",
    "\n",
    "The reverse mode of automatic differentiation recursively propagates the calculated derivative from the left: first calculates $g_3'(.)$, then $g_2'(.)$, then $g_1'(a)$, and so on.\n",
    "\n",
    "In our implementation, we will focus on the forward mode. \n",
    "\n",
    "## Forward Mode\n",
    "\n",
    "A useful tool associated with the forward mode is the computational trace. Using the computational trace, we can list the steps required to go from input values (the point at which the derivative is evaluated) to the input function.  \n",
    "\n",
    "Consider the following function:\n",
    "\n",
    "$$ f(x) = sin(e^{2x}) $$\n",
    "\n",
    "Say we want to evaluate the derivative of this function at $x = 5$. The steps for calculating the derivative using forward mode are given in the following table:\n",
    "\n",
    "| Trace | Elementary Function | Function Value | Derivative | Derivative Value |\n",
    "| :------: | :----------: | :-------: | :---------: | :--------: |\n",
    "| $x_{1}$ | $x$ | 5 | $\\dot{x}_{1}$ | $1$ |\n",
    "| $x_{2}$ | $2x_{1}$ | $10$ | $2\\dot{x}_{1}$ | $2$ |\n",
    "| $x_{3}$ | $e^{x_{2}}$ | $e^{10}$ | $e^{x_{2}}\\dot{x}_{2}$ | $2e^{10}$ |\n",
    "| $x_{4}$ | $sin(x_{3})$ | $sin(e^{10})$ | $cos(x_{3})\\dot{x}_{3}$ | $2e^{10}cos(e^{10})$ |\n",
    "\n",
    "We get the required derivative value $2e^{10}cos(e^{10})$.\n",
    "\n",
    "## Dual Numbers\n",
    "\n",
    "We utilize [dual numbers](https://en.wikipedia.org/wiki/Dual_number) in our implementation of the forward mode of automatic differentiation. Dual numbers are an extension to real numbers (similar to complex numbers). Dual numbers introduce a new element (typically represented by $\\epsilon$) with the useful property $\\epsilon^2 = 0$.  Using dual numbers and [Taylor series](https://en.wikipedia.org/wiki/Taylor_series), we can find the derivative of a function quickly. \n",
    "\n",
    "Say we have a function $f$ and we want to find its derivative at point $x = a$. We will set $x = a + b\\epsilon$ and find the Taylor expansion:\n",
    "\n",
    "$$ f(a+b\\epsilon) = \\sum_{n=0}^{\\infty} \\frac{(b\\epsilon)^nf^{(n)}(a)}{n!} = f(a) + b\\epsilon f'(a) $$\n",
    "\n",
    "All higher-order terms are equal to $0$ because of the dual number property $\\epsilon^2 = 0$. For function $f$, we get its derivative at point $x = a$ directly from the second term in the Taylor expansion.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MQHuPuTFL8Z1"
   },
   "source": [
    "# How To Use\n",
    "\n",
    "## Approach\n",
    "In thinking through how to use our software package, we considered the following:\n",
    "\n",
    "#### Who are our users?\n",
    "We decided to target somewhat tech-savvy users who would be comfortable with a command line application as opposed to a more approachable and user-friendly GUI. Any single user should be able to install the package easily onto their machine without requiring broader deployment. \n",
    "\n",
    "#### Where should it run? \n",
    "We expect our package to be installed and run on any desktop devices through the command line. It should work regardless of the operating system as long as the user has Python on their machine (our assumption is that most users will have Python pre-installed on their Mac or Linux machines or will be able to easily download it otherwise).\n",
    "\n",
    "## Packaging Choice\n",
    "We plan to use pip as the package manager. We considered some other options such as conda install but ultimately chose pip because Python developers are already familiar with it and we did not want the installation process to be a barrier to using our program. We plan to register the name of our package on PyPI and upload source distributions via the setup.py file. To ensure that our package can be downloaded regardless of the user’s Python version, we plan to use pip instead of pip3. \n",
    "\n",
    "## Installation via GitHub\n",
    "\n",
    "To install our package from GitHub, a user should enter the following in the command line:\n",
    "\n",
    "1. Create a new virtual environment (using Conda):\n",
    "\n",
    "`conda create --name test`\n",
    "\n",
    "2. Activate virtual environment:\n",
    "\n",
    "`conda activate test`\n",
    "\n",
    "3. Clone our source code:\n",
    "\n",
    "`git clone https://github.com/make-AD-ifference/cs207-FinalProject.git`\n",
    "\n",
    "4. Change current directory to the cloned repo:\n",
    "\n",
    "`cd cs207-FinalProject/`\n",
    "\n",
    "5. Install dependencies:\n",
    "\n",
    "`pip install -r requirements.txt`\n",
    "\n",
    "6. Start up Python and import AutoDiff class:\n",
    "\n",
    "`>>> python`\n",
    "\n",
    "`>>> from autodiff.autodiff import AutoDiff`\n",
    "\n",
    "`>>> x = AutoDiff(2,3)`\n",
    "\n",
    "`>>> f = x**2`\n",
    "\n",
    "`>>> f`\n",
    "\n",
    "`AutoDiff(4,12)`\n",
    "\n",
    "**[See Implementation section below for further details on using AutoDiff]**\n",
    "\n",
    "7. Finally, quit Python and deactivate environment:\n",
    "\n",
    "`>>>exit()`\n",
    "\n",
    "`conda deactivate`\n",
    "\n",
    "\n",
    "## Installation via pip (to be implemented)\n",
    "\n",
    "To install our package, a user should enter the following in the command line:\n",
    "\n",
    "`pip install make_ad_ifference` \n",
    "(assuming \"make_ad_ifference\" is our root directory name)\n",
    "\n",
    "`import autodiff`\n",
    "(\"autodiff\" is a placeholder for the name of our main sub directory that will contain all our classes)\n",
    "\n",
    "Users will then have access to all the functions that are part of the sub directory. \n",
    "\n",
    "**[We have not released our package on PyPI yet as per instructions for Milestone 2. We will update and finalize this section once the package is released on PyPI.]**\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pTTAsbqkRJTF"
   },
   "source": [
    "# Software Organization\n",
    "\n",
    "\n",
    "## Directory Structure\n",
    "\n",
    "The directory structure will look like:\n",
    "\n",
    "`make-AD-ifference/`\n",
    "\n",
    ">`setup.py`\n",
    "\n",
    ">`.gitignore`\n",
    "\n",
    ">`.travis.yml`\n",
    "\n",
    ">`.requirements.txt`\n",
    "\n",
    ">`coverage.txt`\n",
    "\n",
    ">`README.md`\n",
    "\n",
    ">`LICENSE`\n",
    "\n",
    ">`__init__.py`\n",
    "\n",
    "> `demo.py`\n",
    "\n",
    ">`autodiff/`\n",
    "\n",
    ">>`autodiff.py`\n",
    "\n",
    ">`docs/`\n",
    "\n",
    ">>`milestone1.ipynb`\n",
    "\n",
    ">>`milestone2.ipynb`\n",
    "\n",
    ">`tests/`\n",
    "\n",
    ">>`test_newton.py`\n",
    "\n",
    ">>`test_autodiff.py`\n",
    "\n",
    ">`scratch/`\n",
    "\n",
    ">>`dual.py`\n",
    "\n",
    ">>`test_dual.py`\n",
    "\n",
    "## Modules\n",
    "- `autodiff.py`: contains the autodifferentiation class `AutoDiff`, which implements Forward Mode AD. This module will serve as our custom library that allows users to evaluate functions and their derivatives for each input value. The class also includes custom methods that our program will support. Each method returns the value and derivative of the specified function. See the implementation section for more details. \n",
    "\n",
    "- `test_autodiff.py`: contains our test suite for the `autodiff.py` module. It currently includes tests for single input and scalar functions but will be expanded in the future when our program supports more complex functions such as multi-variable functions.\n",
    "\n",
    "- `test_newton.py`: contains our script for testing the use of our `AutoDiff` class for finding the roots of a functions using Newton's root finding method.\n",
    "\n",
    "- `demo.py`: contains a demonstration of the newton root finding algorithm using both dunder method operations and non-build-in operations. Examples used are $x^2$ and $sin(x) + cos(x)$.\n",
    "\n",
    "\n",
    "## Testing\n",
    "\n",
    "All testing will be done using TravisCI, and evaluation of the tests will be done using CodeCov. Our main test suite `test_autodiff.py` currently lives in the `tests/` folder and any additional testing suites will be added to the same folder. \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q8-KmCY8xFyT"
   },
   "source": [
    "# Implementation\n",
    "\n",
    "\n",
    "## Class: AutoDiff\n",
    "\n",
    "We will use a simple one-class structure to implement forward mode autodifferentiation. The AutoDiff class will keep track of the trace value and the derivative of functions for each input variable. It will have two attributes `val` and `der` which represent the value and the derivative respecitvely. We will have a set of elementary functions that can easily be expanded. These elementary functions will serve as building blocks for users to assemble the function they wish to evaluate.\n",
    "\n",
    "We’ll provide users with an understandable interface that conveys which elementary functions our software supports. If a user enters a function that is a combination of any of these, our program will be able to handle the input. Otherwise, we will return a descriptive error and allow users to enter a new function. We will expand this class in the future to include additional methods and support for more complex functions e.g. multi-variable functions.\n",
    "\n",
    "Elementary functions defined in the AutoDiff class:\n",
    "- Ln (natural log)\n",
    "- Log \n",
    "- Exp\n",
    "- Sin\n",
    "- Cos\n",
    "- Tan\n",
    "- Sqrt\n",
    "\n",
    "Elementary mathematical operations supported (note that the cummutative properties of operations are preserved):\n",
    "- Addition\n",
    "- Subtraction\n",
    "- Multiplication\n",
    "- Division\n",
    "- Power\n",
    "\n",
    "\n",
    "## More Details about Attributes & Methods\n",
    "We have overloaded the methods in class `AutoDiff` to give the user flexibility in how functions are entered. Overloaded functions will support elementary operations between two `AutoDiff` objects or 1 `AutoDiff` object and 1 scalar. This way, we also preserve the cummutative nature of functions where needed. As an example, the user may input f = 2x + 3 or f = 3 + 2x. Regardless of the order, will be able to return the correct value and derivative. \n",
    "Each `AutoDiff` object will have as attributes the value and the derivative, calculated using trace variables and elementary operations, for a given input value. \n",
    "\n",
    "More on the custom operations and functions supported in class `AutoDiff`:\n",
    "- `__init__`:  constructor of class `AutoDiff`. Initializes an `AutoDiff` object, setting `self.der` initially to 1. Takes in an input value `val` at which to evaluate the function value and derivative.\n",
    "- `__add__`: overloaded addition function. Supports adding two `AutoDiff` objects or 1 `AutoDiff` object and 1 scalar.\n",
    "- `__str__`: returns the string value of the function.\n",
    "- `__repr__`: returns the string value of the function.\n",
    "- `__radd__`: Supports addition of two `AutoDiff` objects or 1 `AutoDiff` object and 1 scalar regardless of input order. Ensures cummutative property of addition is preserved.\n",
    "- `__sub__`: overloaded subtraction function. Supports subtraction between two `AutoDiff` objects or 1 `AutoDiff` object and 1 scalar.\n",
    "- `__rsub__`: Supports subtraction of the form scalar - `AutoDiff` instead of `AutoDiff` - scalar.\n",
    "- `__mul__`: overloaded multiplication function. Supports multiplying two `AutoDiff` objects or 1 `AutoDiff` object and 1 scalar.\n",
    "- `__rmul__`: overloaded multiplication function. Supports multiplying two `AutoDiff` objects or 1 `AutoDiff` object and 1 scalar regardless of input order. Ensures cummutative property of multiplication is preserved.\n",
    "- `__truediv__`: overloaded division function. Supports dividing an `AutoDiff` object by another `AutoDiff` object or an `AutoDiff` object by a scalar.\n",
    "- `__rtruediv__`: Supports division of the form scalar / `AutoDiff` instead of `AutoDiff` / scalar.\n",
    "- `__pow__`: overloaded power function. Supports an `AutoDiff` object to the power of another `AutoDiff` object or an `AutoDiff` object to the power of a scalar.\n",
    "- `__neg__`: returns negated `AutoDiff` object.\n",
    "- `sin`: returns sine of `AutoDiff` object.\n",
    "- `cos`: returns cosine of `AutoDiff` object.\n",
    "- `tan`: returns tangent of `AutoDiff` object.\n",
    "- `ln`: returns natural log of `AutoDiff` object.\n",
    "- `log`: returns log of `AutoDiff` object with base `base` as second input.\n",
    "- `exp`: returns exponential of `AutoDiff` object.\n",
    "- `sqrt`: returns square root of `AutoDiff` object.\n",
    "\n",
    "## Using AutoDiff\n",
    "\n",
    "- Begin by initializing an AutoDiff object with a given value and derivation:\n",
    "\n",
    "`x = AutoDiff(2,3)`\n",
    "\n",
    "- Then define the function in the following manner:\n",
    "\n",
    "`f = x**2`\n",
    "\n",
    "`f = AutoDiff.sin(x)`\n",
    "\n",
    "`f = AutoDiff.log(x,2)`\n",
    "\n",
    "- The function's value can then be accessed as `f.val`\n",
    "\n",
    "- The function's derivative can then be access as `f.der`\n",
    "\n",
    "**[We will further develop this section after we implement handling of vectors.]**\n",
    "\n",
    "## External Dependencies\n",
    "- `Numpy`: In order to run our program, users will have to import the numpy library as follows:\n",
    "`import numpy as np`. The Numpy library provides support for the elementary mathematical functions and operations handled by our program. As such, we have included `numpy` in the `requirements.txt` file.\n",
    "\n",
    "## Efficiency\n",
    "\n",
    "We will have to consider things such as memory accesses, which can greatly speed up or slow down the code. We will also have to consider numerical precision, although this shouldn’t be an issue with our hard-coding of functions and derivatives. Another possible consideration is memory overhead. Efficient storage of the functions and evaluations is crucial to making the code usable.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6E-SCpW7Hl0t"
   },
   "source": [
    "# Future Work\n",
    "\n",
    "## Future Implementations\n",
    "\n",
    "In the future our package will be extended to handle vector valued functions.\n",
    "Generally, we will be able to handle functions of the form\n",
    "\n",
    "$$ f: \\mathbb{R}^m \\to \\mathbb{R}^n $$\n",
    "\n",
    "for arbitrary $m$ and $n$.\n",
    "\n",
    "In order to accomplish this we will need the Jacobian matrix\n",
    "\n",
    "$$\\textbf{J} = \\left[\\begin{matrix} \\frac{\\partial f_1}{\\partial x_1} & \\cdots  & \\frac{\\partial f_1}{\\partial x_n} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial f_m}{\\partial x_1} & \\cdots & \\frac{\\partial f_m}{\\partial x_n}  \\end{matrix} \\right]$$\n",
    "Our current implementation handles functions of the form \n",
    "$$ f: \\mathbb{R} \\to \\mathbb{R}^n .$$\n",
    "This needs to be expanded to multiple inputs, which can most likely be implemented by modifying each function in the `AutoDiff` class to return partial derivatives with respect to the inputs as vectors, rather than just scalars.\n",
    "\n",
    "For example the adding function would be modified from this:\n",
    "\n",
    "\n",
    "`def __add__(self, other):`\n",
    "\n",
    ">`try:`\n",
    "\n",
    ">>`new_val = self.val + other.val`\n",
    "\n",
    ">>`new_der = self.der + other.der`\n",
    "\n",
    ">`except AttributeError:`\n",
    "\n",
    ">>`new_val = self.val + other`\n",
    "\n",
    ">>`new_der = self.der`\n",
    "\n",
    "> `return AutoDiff(new_val, new_der)`\n",
    "\n",
    "to be something along the lines of:\n",
    "\n",
    "`def __add__(self, other):`\n",
    "\n",
    ">`try:`\n",
    "\n",
    ">>`new_val = self.val + other.val`\n",
    "\n",
    ">>`if(isinstance(other, AutoDiff):`\n",
    "\n",
    ">>>`new_der = [self.der, other.der]`\n",
    "\n",
    ">>`else:`\n",
    "\n",
    ">>>`new_der = self.der + other.der`\n",
    "\n",
    ">`except AttributeError:`\n",
    "\n",
    ">>`new_val = self.val + other`\n",
    "\n",
    ">>`new_der = self.der`\n",
    "\n",
    "> `return AutoDiff(new_val, new_der)`\n",
    "\n",
    "Note: this specific implementation has not been tested yet, but this is the idea our first try will be based on.\n",
    "\n",
    "## Extensions\n",
    "\n",
    "Possible future extensions are to implement the reverse mode, or a root finding and optimization suite.\n",
    "The root finding suite would employ a variety of derivative based root-finding methods such as Newton's method, fixed-point iteration, Laguerre's method, etc.\n",
    "Adding a root finding suite is relatively straightforward.\n",
    "We would add an additional directory within `autodiff/` called `root_finding/` that will contain scripts for running each method.\n",
    "The optimization suite will be inside the `autodiff/optimization/` directory.\n",
    "Possible algorithms to be implemented are gradient descent, stochastic gradient descent, constrained optimization, Newton's method, etc.\n",
    "Each method will get its own script. \n",
    "For example, Newton's method would live in `/makeADifference/autodiff/root_finding/newton.py`.\n",
    "\n",
    "The reverse mode implementation is trickier and not finalized at this point.\n",
    "The plan would be to specify in the `AutoDiff` object which mode, forward or reverse, we would want to compute, potentially default to forward mode with the option to specify reverse mode.\n",
    "This is because the different modes are utilized for different situations, and automatically computing one defeats the purpose of having the other.\n",
    "Automatic calling of one mode or the other can also lead to inefficiency.\n",
    "In order to actually compute the reverse mode, we will need to hold on to the evaluation trace and derivatives, which can potentially be accomplished by using a flag in each function to return the previous derivatives and values as well as current derivatives and values.\n",
    "This would look something like\n",
    "\n",
    "`def __exp__(self, reverse=False):`\n",
    "> `new_val = np.exp(self.val)`\n",
    "\n",
    "> `new_der = np.exp(self.der)`\n",
    "\n",
    ">`if(reverse):`\n",
    "\n",
    ">> `self.previous.append(new_val, new_der)`\n",
    "\n",
    "\n",
    "\n",
    "> `return AutoDiff(new_val, new_der)`\n",
    "where `self.previous` is a list containing the derivatives and values of the evaluation trace.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aAvFPve-auXg"
   },
   "source": [
    "# Resources\n",
    "\n",
    "- David Sondak, *CS 207 Lectures* (https://harvard-iacs.github.io/2019-CS207/lectures/)\n",
    "- Philipp Hoffmann, *A Hitchhiker’s Guide to Automatic Differentiation* (https://doi.org/10.1007/s11075-015-0067-6)\n",
    "- Richard D. Neidinger, *Introduction to Automatic Differentiation and MATLAB Object-Oriented Programming* (https://www.neidinger.net/SIAMRev74362.pdf)\n",
    "- Baydin, et al., *Automatic Differentiation in Machine Learning: a Survey*\n",
    "(https://arxiv.org/pdf/1502.05767.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "milestone2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
